#!/usr/bin/env python3
"""
æ–°ç”¨æˆ·æ•°æ®å…¼å®¹æ€§æ£€æŸ¥å·¥å…·

è¯¥è„šæœ¬ç”¨äºæ£€æŸ¥æ–°ç”¨æˆ·æ•°æ®ä¸è®­ç»ƒæ•°æ®çš„å…¼å®¹æ€§ï¼Œ
æä¾›è¯¦ç»†çš„è¿‡æ»¤æŠ¥å‘Šå’Œæ•°æ®è´¨é‡åˆ†æã€‚

ä½¿ç”¨æ–¹æ³•:
    python check_new_user_data.py --experiment_name your_experiment
    python check_new_user_data.py --experiment_name your_experiment --new_user_behavior_path data/custom_behavior.csv
"""

import argparse
import os
import pandas as pd
import pickle
from collections import defaultdict
import sys

def load_training_entities(experiment_path):
    """åŠ è½½è®­ç»ƒå®ä½“è®°å½•"""
    entities_path = os.path.join(experiment_path, 'processed_data', 'training_entities.pkl')
    if os.path.exists(entities_path):
        with open(entities_path, 'rb') as f:
            return pickle.load(f)
    else:
        print(f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°è®­ç»ƒå®ä½“è®°å½•æ–‡ä»¶: {entities_path}")
        return None

def analyze_behavior_data(behavior_path, training_entities):
    """åˆ†æè¡Œä¸ºæ•°æ®å…¼å®¹æ€§"""
    print("ğŸ” åˆ†æè¡Œä¸ºæ•°æ®...")
    
    if not os.path.exists(behavior_path):
        print(f"âŒ è¡Œä¸ºæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {behavior_path}")
        return None
    
    df = pd.read_csv(behavior_path)
    print(f"ğŸ“ åŠ è½½è¡Œä¸ºæ•°æ®: {behavior_path}")
    print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
    
    # ç»Ÿè®¡URL
    all_urls = set(df['url'].unique())
    training_urls = training_entities['urls']
    
    known_urls = all_urls.intersection(training_urls)
    unknown_urls = all_urls - training_urls
    
    # æŒ‰ç”¨æˆ·ç»Ÿè®¡
    user_stats = {}
    for user_id, group in df.groupby('user_id'):
        user_urls = set(group['url'])
        user_known = user_urls.intersection(training_urls)
        user_unknown = user_urls - training_urls
        
        user_stats[user_id] = {
            'total_records': len(group),
            'total_urls': len(user_urls),
            'known_urls': len(user_known),
            'unknown_urls': len(user_unknown),
            'known_records': len(group[group['url'].isin(training_urls)]),
            'unknown_records': len(group[~group['url'].isin(training_urls)]),
            'coverage': len(user_known) / len(user_urls) if user_urls else 0
        }
    
    return {
        'total_urls': len(all_urls),
        'known_urls': len(known_urls),
        'unknown_urls': len(unknown_urls),
        'unknown_url_list': sorted(unknown_urls),
        'user_stats': user_stats,
        'coverage': len(known_urls) / len(all_urls) if all_urls else 0
    }

def analyze_location_data(location_path, training_entities):
    """åˆ†æä½ç½®æ•°æ®å…¼å®¹æ€§"""
    print("ğŸ” åˆ†æä½ç½®æ•°æ®...")
    
    if not os.path.exists(location_path):
        print(f"âŒ ä½ç½®æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {location_path}")
        return None
    
    if 'base_stations' not in training_entities:
        print("âš ï¸ è®­ç»ƒæ•°æ®ä¸­æ²¡æœ‰åŸºç«™ä¿¡æ¯")
        return None
    
    df = pd.read_csv(location_path, sep='\t')
    print(f"ğŸ“ åŠ è½½ä½ç½®æ•°æ®: {location_path}")
    print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
    
    # ç»Ÿè®¡åŸºç«™
    all_base_stations = set(df['base_station_id'].unique())
    training_base_stations = training_entities['base_stations']
    
    known_base_stations = all_base_stations.intersection(training_base_stations)
    unknown_base_stations = all_base_stations - training_base_stations
    
    # æŒ‰ç”¨æˆ·ç»Ÿè®¡
    user_stats = {}
    for user_id, group in df.groupby('user_id'):
        user_base_stations = set(group['base_station_id'])
        user_known = user_base_stations.intersection(training_base_stations)
        user_unknown = user_base_stations - training_base_stations
        
        user_stats[user_id] = {
            'total_records': len(group),
            'total_base_stations': len(user_base_stations),
            'known_base_stations': len(user_known),
            'unknown_base_stations': len(user_unknown),
            'known_records': len(group[group['base_station_id'].isin(training_base_stations)]),
            'unknown_records': len(group[~group['base_station_id'].isin(training_base_stations)]),
            'coverage': len(user_known) / len(user_base_stations) if user_base_stations else 0
        }
    
    return {
        'total_base_stations': len(all_base_stations),
        'known_base_stations': len(known_base_stations),
        'unknown_base_stations': len(unknown_base_stations),
        'unknown_base_station_list': sorted(unknown_base_stations),
        'user_stats': user_stats,
        'coverage': len(known_base_stations) / len(all_base_stations) if all_base_stations else 0
    }

def print_compatibility_report(behavior_analysis, location_analysis):
    """æ‰“å°å…¼å®¹æ€§æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“‹ æ–°ç”¨æˆ·æ•°æ®å…¼å®¹æ€§æŠ¥å‘Š")
    print("="*60)
    
    # æ€»ä½“å…¼å®¹æ€§è¯„åˆ†
    total_score = 0
    max_score = 0
    
    # è¡Œä¸ºæ•°æ®åˆ†æ
    if behavior_analysis:
        print(f"\nğŸŒ è¡Œä¸ºæ•°æ®åˆ†æ:")
        print(f"  ğŸ“Š URLæ€»æ•°: {behavior_analysis['total_urls']}")
        print(f"  âœ… å·²çŸ¥URL: {behavior_analysis['known_urls']}")
        print(f"  âŒ æœªçŸ¥URL: {behavior_analysis['unknown_urls']}")
        print(f"  ğŸ“ˆ è¦†ç›–ç‡: {behavior_analysis['coverage']*100:.1f}%")
        
        total_score += behavior_analysis['coverage'] * 50
        max_score += 50
        
        if behavior_analysis['unknown_urls'] > 0:
            print(f"  ğŸ“‹ æœªçŸ¥URLåˆ—è¡¨ (å‰10ä¸ª): {behavior_analysis['unknown_url_list'][:10]}")
            if len(behavior_analysis['unknown_url_list']) > 10:
                print(f"      ... è¿˜æœ‰ {len(behavior_analysis['unknown_url_list'])-10} ä¸ª")
        
        # ç”¨æˆ·çº§åˆ«ç»Ÿè®¡
        print(f"\nğŸ‘¥ ç”¨æˆ·çº§åˆ«åˆ†æ:")
        for user_id, stats in behavior_analysis['user_stats'].items():
            print(f"  {user_id}: {stats['known_records']}/{stats['total_records']} è®°å½•å¯ç”¨ "
                  f"({stats['coverage']*100:.1f}% URLè¦†ç›–ç‡)")
    
    # ä½ç½®æ•°æ®åˆ†æ
    if location_analysis:
        print(f"\nğŸ“¡ ä½ç½®æ•°æ®åˆ†æ:")
        print(f"  ğŸ“Š åŸºç«™æ€»æ•°: {location_analysis['total_base_stations']}")
        print(f"  âœ… å·²çŸ¥åŸºç«™: {location_analysis['known_base_stations']}")
        print(f"  âŒ æœªçŸ¥åŸºç«™: {location_analysis['unknown_base_stations']}")
        print(f"  ğŸ“ˆ è¦†ç›–ç‡: {location_analysis['coverage']*100:.1f}%")
        
        total_score += location_analysis['coverage'] * 50
        max_score += 50
        
        if location_analysis['unknown_base_stations'] > 0:
            print(f"  ğŸ“‹ æœªçŸ¥åŸºç«™åˆ—è¡¨: {location_analysis['unknown_base_station_list']}")
        
        # ç”¨æˆ·çº§åˆ«ç»Ÿè®¡
        print(f"\nğŸ‘¥ ç”¨æˆ·çº§åˆ«åˆ†æ:")
        for user_id, stats in location_analysis['user_stats'].items():
            print(f"  {user_id}: {stats['known_records']}/{stats['total_records']} è®°å½•å¯ç”¨ "
                  f"({stats['coverage']*100:.1f}% åŸºç«™è¦†ç›–ç‡)")
    
    # æ€»ä½“è¯„åˆ†
    if max_score > 0:
        final_score = total_score / max_score
        print(f"\nğŸ¯ æ€»ä½“å…¼å®¹æ€§è¯„åˆ†: {final_score*100:.1f}%")
        
        if final_score >= 0.8:
            print("âœ… ä¼˜ç§€ - æ•°æ®å…¼å®¹æ€§å¾ˆå¥½ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨")
        elif final_score >= 0.6:
            print("âš ï¸ è‰¯å¥½ - æ•°æ®åŸºæœ¬å…¼å®¹ï¼Œä½†å»ºè®®æ£€æŸ¥æœªçŸ¥å®ä½“")
        elif final_score >= 0.4:
            print("âš ï¸ ä¸€èˆ¬ - æ•°æ®å…¼å®¹æ€§è¾ƒå·®ï¼Œå»ºè®®è¡¥å……è®­ç»ƒæ•°æ®")
        else:
            print("âŒ è¾ƒå·® - æ•°æ®å…¼å®¹æ€§å¾ˆå·®ï¼Œéœ€è¦é‡æ–°å‡†å¤‡æ•°æ®")
    
    # å»ºè®®
    print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
    if behavior_analysis and behavior_analysis['unknown_urls'] > 0:
        print(f"  â€¢ è€ƒè™‘å°†é‡è¦çš„æœªçŸ¥URLæ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­")
        print(f"  â€¢ æˆ–è€…å°†æœªçŸ¥URLæ˜ å°„åˆ°ç›¸ä¼¼çš„å·²çŸ¥URL")
    
    if location_analysis and location_analysis['unknown_base_stations'] > 0:
        print(f"  â€¢ æ£€æŸ¥åŸºç«™IDçš„å‘½åè§„èŒƒæ˜¯å¦ä¸€è‡´")
        print(f"  â€¢ è€ƒè™‘å°†é‡è¦çš„æœªçŸ¥åŸºç«™æ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­")
    
    print("="*60)

def main():
    parser = argparse.ArgumentParser(description='æ–°ç”¨æˆ·æ•°æ®å…¼å®¹æ€§æ£€æŸ¥å·¥å…·')
    parser.add_argument('--experiment_name', type=str, required=True, 
                       help='å®éªŒåç§°')
    parser.add_argument('--experiment_path', type=str, 
                       help='å®éªŒè·¯å¾„ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œå°†ä½¿ç”¨experiments/{experiment_name}ï¼‰')
    parser.add_argument('--new_user_behavior_path', type=str, 
                       default='data/new_user_behavior.csv',
                       help='æ–°ç”¨æˆ·è¡Œä¸ºæ•°æ®è·¯å¾„')
    parser.add_argument('--new_user_location_path', type=str, 
                       default='data/new_user_base_stations.tsv',
                       help='æ–°ç”¨æˆ·ä½ç½®æ•°æ®è·¯å¾„')
    
    args = parser.parse_args()
    
    # ç¡®å®šå®éªŒè·¯å¾„
    if args.experiment_path:
        experiment_path = args.experiment_path
    else:
        experiment_path = f'experiments/{args.experiment_name}'
    
    if not os.path.exists(experiment_path):
        print(f"âŒ é”™è¯¯ï¼šå®éªŒè·¯å¾„ä¸å­˜åœ¨: {experiment_path}")
        sys.exit(1)
    
    print(f"ğŸ” æ£€æŸ¥å®éªŒ: {args.experiment_name}")
    print(f"ğŸ“ å®éªŒè·¯å¾„: {experiment_path}")
    print(f"ğŸ“± è¡Œä¸ºæ•°æ®: {args.new_user_behavior_path}")
    print(f"ğŸ“ ä½ç½®æ•°æ®: {args.new_user_location_path}")
    
    # åŠ è½½è®­ç»ƒå®ä½“è®°å½•
    training_entities = load_training_entities(experiment_path)
    if training_entities is None:
        print("âŒ æ— æ³•åŠ è½½è®­ç»ƒå®ä½“è®°å½•ï¼Œè¯·ç¡®ä¿å·²å®Œæˆæ•°æ®é¢„å¤„ç†")
        sys.exit(1)
    
    print(f"âœ… æˆåŠŸåŠ è½½è®­ç»ƒå®ä½“è®°å½•:")
    print(f"  ğŸ“Š è®­ç»ƒURLæ•°é‡: {len(training_entities['urls'])}")
    if 'base_stations' in training_entities:
        print(f"  ğŸ“¡ è®­ç»ƒåŸºç«™æ•°é‡: {len(training_entities['base_stations'])}")
    
    # åˆ†æè¡Œä¸ºæ•°æ®
    behavior_analysis = analyze_behavior_data(args.new_user_behavior_path, training_entities)
    
    # åˆ†æä½ç½®æ•°æ®
    location_analysis = analyze_location_data(args.new_user_location_path, training_entities)
    
    # æ‰“å°æŠ¥å‘Š
    print_compatibility_report(behavior_analysis, location_analysis)

if __name__ == "__main__":
    main() 