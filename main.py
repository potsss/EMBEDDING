"""
ä¸»ç¨‹åº
æ•´åˆæ‰€æœ‰æ¨¡å—ï¼Œæä¾›å®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹
"""
import os
import argparse
import torch
import numpy as np
import random
import json
import pickle
from datetime import datetime
from config import Config, get_experiment_dir, get_experiment_paths
from data_preprocessing import DataPreprocessor, LocationProcessor
import pandas as pd
from model import Item2Vec, UserEmbedding
from trainer import Trainer, AttributeTrainer, train_location_model
from trainer import load_attribute_models
from visualizer import Visualizer

# å¯¼å…¥Node2Vecç›¸å…³çš„æ¨¡å—
from model import Node2Vec
from utils.node2vec_utils import build_graph_from_sequences, generate_node2vec_walks, generate_node2vec_walks_precompute, generate_node2vec_walks_with_cache

# å¯¼å…¥å±æ€§ç›¸å…³çš„æ¨¡å—
from model import AttributeEmbeddingModel, UserFusionModel, EnhancedUserEmbedding

# å¯¼å…¥ä½ç½®ç›¸å…³çš„æ¨¡å—
from model import UserLocationEmbedding

def set_random_seed(seed):
    """
    è®¾ç½®éšæœºç§å­
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

def save_training_entities(url_mappings, base_station_mappings, processed_data_path):
    """
    ä¿å­˜è®­ç»ƒæ—¶çš„å®ä½“è®°å½•ï¼Œç”¨äºæ–°ç”¨æˆ·æ¨ç†æ—¶çš„è¿‡æ»¤
    
    Args:
        url_mappings: URLæ˜ å°„å­—å…¸
        base_station_mappings: åŸºç«™æ˜ å°„å­—å…¸ï¼ˆå¯é€‰ï¼‰
        processed_data_path: å¤„ç†æ•°æ®ä¿å­˜è·¯å¾„
    """
    training_entities = {
        'urls': set(url_mappings['url_to_id'].keys()),
        'url_to_id': url_mappings['url_to_id'],
        'id_to_url': url_mappings['id_to_url']
    }
    
    # æ·»åŠ åŸºç«™ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if base_station_mappings:
        training_entities['base_stations'] = set(base_station_mappings['base_station_to_id'].keys())
        training_entities['base_station_to_id'] = base_station_mappings['base_station_to_id']
        training_entities['id_to_base_station'] = base_station_mappings['id_to_base_station']
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    entities_path = os.path.join(processed_data_path, 'training_entities.pkl')
    with open(entities_path, 'wb') as f:
        pickle.dump(training_entities, f)
    
    print(f"è®­ç»ƒå®ä½“è®°å½•å·²ä¿å­˜: {entities_path}")
    print(f"  è®­ç»ƒURLæ•°é‡: {len(training_entities['urls'])}")
    if base_station_mappings:
        print(f"  è®­ç»ƒåŸºç«™æ•°é‡: {len(training_entities['base_stations'])}")

def load_training_entities(processed_data_path):
    """
    åŠ è½½è®­ç»ƒæ—¶çš„å®ä½“è®°å½•
    
    Args:
        processed_data_path: å¤„ç†æ•°æ®è·¯å¾„
        
    Returns:
        è®­ç»ƒå®ä½“è®°å½•å­—å…¸
    """
    entities_path = os.path.join(processed_data_path, 'training_entities.pkl')
    if os.path.exists(entities_path):
        with open(entities_path, 'rb') as f:
            return pickle.load(f)
    else:
        print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°è®­ç»ƒå®ä½“è®°å½•æ–‡ä»¶: {entities_path}")
        return None

def initialize_experiment_paths(experiment_name_override=None, mode=None):
    """
    åˆå§‹åŒ–å®éªŒç›¸å…³çš„è·¯å¾„ï¼Œå¹¶è®¾ç½®åˆ°Configç±»ä¸Šã€‚
    æ ¹æ®è¿è¡Œæ¨¡å¼å†³å®šæ˜¯å¦å…è®¸ä½¿ç”¨å·²å­˜åœ¨çš„ä¸å¸¦æ—¶é—´æˆ³çš„ç›®å½•ã€‚
    """
    current_experiment_name = experiment_name_override if experiment_name_override else Config.EXPERIMENT_NAME
    
    force_recalc = True # é»˜è®¤ä¸ºTrueï¼Œç¡®ä¿åœ¨æ–°çš„main.pyæ‰§è¡Œå¼€å§‹æ—¶é‡æ–°è®¡ç®—
    allow_existing = False

    if experiment_name_override:
        # å¦‚æœé€šè¿‡å‘½ä»¤è¡ŒæŒ‡å®šäº†å®éªŒåç§°
        Config.EXPERIMENT_NAME = experiment_name_override
        # å¯¹äºæ¨ç†ç›¸å…³æ¨¡å¼ï¼Œå…è®¸ä½¿ç”¨å·²å­˜åœ¨çš„ç›®å½•
        if mode in ['compute_new_users', 'visualize', 'compute_embeddings']:
            allow_existing = True
        else:
            allow_existing = False 
    elif mode and mode not in ['preprocess', 'all']:
        # å¦‚æœæ˜¯åˆ†æ­¥æ‰§è¡Œï¼ˆépreprocess/allï¼‰ï¼Œä¸”æœªæŒ‡å®šæ–°å®éªŒåï¼Œåˆ™å°è¯•ä½¿ç”¨å·²å­˜åœ¨çš„ç›®å½•
        allow_existing = True
        force_recalc = True # ä»ç„¶éœ€è¦é‡æ–°è®¡ç®—ï¼Œä½†ä¼šä¼˜å…ˆä½¿ç”¨å·²å­˜åœ¨çš„
    
    paths = get_experiment_paths(current_experiment_name, 
                                 allow_existing_without_timestamp=allow_existing, 
                                 force_recalculate=force_recalc)
    for key, value in paths.items():
        setattr(Config, key, value)
    
    # ç¡®ä¿DEVICE_OBJåœ¨è·¯å¾„ç¡®å®šåè®¾ç½®
    Config.DEVICE_OBJ = torch.device(Config.DEVICE)

def create_directories():
    """
    åˆ›å»ºå¿…è¦çš„ç›®å½•ã€‚æ­¤æ—¶Configä¸­çš„è·¯å¾„åº”è¯¥å·²ç»è¢«æ­£ç¡®è®¾ç½®äº†ã€‚
    """
    # è·å–å·²ç¡®å®šçš„å®éªŒç›®å½•è·¯å¾„
    # æ³¨æ„ï¼šè¿™é‡Œçš„get_experiment_dirè°ƒç”¨ä¸åº”å†ä¿®æ”¹_ACTUAL_EXPERIMENT_DIRï¼Œå› ä¸ºå®ƒåº”è¯¥å·²ç»è¢«initialize_experiment_pathsæ­£ç¡®è®¾ç½®
    # æ‰€ä»¥ï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åº”è¯¥ç›´æ¥ä»Configä¸­è·å–experiment_dirï¼Œæˆ–è€…ç¡®ä¿get_experiment_diråœ¨ä¸å¸¦force_recalculateæ—¶è¿”å›ç¼“å­˜å€¼
    experiment_dir_to_create = get_experiment_dir(Config.EXPERIMENT_NAME) # è¿™ä¼šè¿”å›ç¼“å­˜çš„è·¯å¾„
    
    print(f"\nç¡®ä¿å®éªŒç›®å½•å­˜åœ¨: {experiment_dir_to_create}")
    os.makedirs(experiment_dir_to_create, exist_ok=True)
    
    # å­ç›®å½•ç°åœ¨ç›´æ¥ä»Configä¸­è·å–
    sub_directories = [
        Config.PROCESSED_DATA_PATH,
        Config.CHECKPOINT_DIR,
        Config.MODEL_SAVE_PATH,
        Config.LOG_DIR,
        Config.TENSORBOARD_DIR,
        Config.VISUALIZATION_DIR,
        # Config.DATA_DIR # DATA_DIR é€šå¸¸æ˜¯é¡¹ç›®çº§åˆ«çš„ï¼Œä¸åº”åœ¨æ¯ä¸ªå®éªŒä¸‹åˆ›å»º
    ]
    # ç¡®ä¿é¡¹ç›®çº§æ•°æ®ç›®å½•å­˜åœ¨
    if not os.path.exists(Config.DATA_DIR):
        os.makedirs(Config.DATA_DIR, exist_ok=True)
        print(f"åˆ›å»ºé¡¹ç›®æ•°æ®ç›®å½•: {Config.DATA_DIR}")

    for directory in sub_directories:
        os.makedirs(directory, exist_ok=True)
        print(f"åˆ›å»ºå­ç›®å½•: {directory}")
    
    # ä¿å­˜å®éªŒé…ç½®ä¿¡æ¯
    def convert_to_serializable(obj):
        if isinstance(obj, torch.device):
            return str(obj)
        elif isinstance(obj, (int, float, str, bool, list, dict, tuple)):
            return obj
        elif obj is None:
            return None
        return str(obj)
    
    config_dict = {}
    for key in dir(Config):
        if not key.startswith('_') and not callable(getattr(Config, key)) and key != 'DEVICE_OBJ': # æ’é™¤DEVICE_OBJï¼Œå› ä¸ºå®ƒä¸æ˜¯åŸå§‹é…ç½®é¡¹
            value = getattr(Config, key)
            config_dict[key] = convert_to_serializable(value)
    
    config_info = {
        'experiment_name': Config.EXPERIMENT_NAME,
        'actual_experiment_dir': experiment_dir_to_create, # ä½¿ç”¨å®é™…åˆ›å»º/ä½¿ç”¨çš„ç›®å½•
        'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'config': config_dict
    }
    
    config_path = os.path.join(experiment_dir_to_create, 'experiment_config.json')
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config_info, f, indent=4, ensure_ascii=False)
    
    print(f"\nå®éªŒé…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    print("="*50)

def preprocess_data(data_path=None):
    """
    æ•°æ®é¢„å¤„ç†
    è¿”å›:
        user_sequences: ç”¨æˆ·è®¿é—®åºåˆ—
        url_mappings: åŸŸååˆ°IDçš„æ˜ å°„
        user_attributes: ç”¨æˆ·å±æ€§æ•°æ®
        attribute_info: å±æ€§ä¿¡æ¯
        user_location_sequences: ç”¨æˆ·ä½ç½®åºåˆ—
        base_station_mappings: åŸºç«™åˆ°IDçš„æ˜ å°„
        location_weights: ä½ç½®æƒé‡ä¿¡æ¯
    """
    print("="*50)
    print("å¼€å§‹æ•°æ®é¢„å¤„ç†")
    print("="*50)
    
    preprocessor = DataPreprocessor()
    
    if data_path:
        user_sequences = preprocessor.preprocess(data_path)
    else:
        # å°è¯•åŠ è½½å·²å¤„ç†çš„æ•°æ®
        try:
            user_sequences = preprocessor.load_processed_data()
            print("å·²åŠ è½½å¤„ç†åçš„æ•°æ®")
        except:
            print("æœªæ‰¾åˆ°å¤„ç†åçš„æ•°æ®ï¼Œè¯·æä¾›åŸå§‹æ•°æ®è·¯å¾„")
            return None, None, None, None, None, None, None
    
    url_mappings = {
        'url_to_id': preprocessor.url_to_id,
        'id_to_url': preprocessor.id_to_url
    }
    
    print(f"æ•°æ®é¢„å¤„ç†å®Œæˆ:")
    print(f"  ç”¨æˆ·æ•°é‡: {len(user_sequences)}")
    print(f"  ç‰©å“æ•°é‡: {len(url_mappings['url_to_id'])}")
    
    # ä¿å­˜è®­ç»ƒå®ä½“è®°å½•ï¼ˆåœ¨å±æ€§å’Œä½ç½®æ•°æ®åŠ è½½åè¿›è¡Œï¼‰
    # è¿™é‡Œå…ˆå£°æ˜ï¼Œåé¢ä¼šåœ¨æ‰€æœ‰æ•°æ®åŠ è½½å®Œæˆåè°ƒç”¨
    
    # åŠ è½½å±æ€§æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    user_attributes = None
    attribute_info = None
    if Config.ENABLE_ATTRIBUTES and preprocessor.attribute_processor:
        try:
            user_attributes, attribute_info = preprocessor.attribute_processor.load_processed_attributes()
            if user_attributes is not None:
                print(f"  å±æ€§ç”¨æˆ·æ•°é‡: {len(user_attributes)}")
                print(f"  å±æ€§æ•°é‡: {len(attribute_info)}")
            else:
                print("  å±æ€§æ•°æ®æœªæ‰¾åˆ°æˆ–å¤„ç†å¤±è´¥")
        except:
            print("  æ— æ³•åŠ è½½å±æ€§æ•°æ®")
    
    # åŠ è½½ä½ç½®æ•°æ®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    user_location_sequences = None
    base_station_mappings = None
    location_weights = None
    if Config.ENABLE_LOCATION and preprocessor.location_processor:
        try:
            location_data = preprocessor.location_processor.load_processed_data(Config.PROCESSED_DATA_PATH)
            if location_data:
                user_location_sequences = location_data['user_sequences']
                base_station_mappings = location_data['base_station_mappings']
                location_weights = location_data['user_weights']
                print(f"ä½ç½®æ•°æ®åŠ è½½æˆåŠŸï¼Œç”¨æˆ·æ•°é‡: {len(user_location_sequences)}")
                print(f"  ä½ç½®ç”¨æˆ·æ•°é‡: {len(user_location_sequences)}")
                print(f"  åŸºç«™æ•°é‡: {len(base_station_mappings['base_station_to_id'])}")
            else:
                print("ä½ç½®æ•°æ®åŠ è½½æˆåŠŸï¼Œç”¨æˆ·æ•°é‡: 0")
                print("  ä½ç½®ç”¨æˆ·æ•°é‡: 0")
                print("  åŸºç«™æ•°é‡: 0")
        except Exception as e:
            print(f"ä½ç½®æ•°æ®åŠ è½½æˆåŠŸï¼Œç”¨æˆ·æ•°é‡: 0")
            print(f"  ä½ç½®ç”¨æˆ·æ•°é‡: 0")
            print(f"  åŸºç«™æ•°é‡: 0")
            print(f"  æ— æ³•åŠ è½½ä½ç½®æ•°æ®: {e}")
    
    # ä¿å­˜è®­ç»ƒå®ä½“è®°å½•ï¼ˆç”¨äºæ–°ç”¨æˆ·æ¨ç†æ—¶çš„è¿‡æ»¤ï¼‰
    save_training_entities(url_mappings, base_station_mappings, Config.PROCESSED_DATA_PATH)
    
    return user_sequences, url_mappings, user_attributes, attribute_info, user_location_sequences, base_station_mappings, location_weights

def train_model(user_sequences, url_mappings, resume=False):
    """
    è®­ç»ƒæ¨¡å‹
    """
    print("="*50)
    print("å¼€å§‹æ¨¡å‹è®­ç»ƒ")
    print("="*50)
    
    # åˆ›å»ºæ¨¡å‹
    vocab_size = len(url_mappings['url_to_id'])
    model = Item2Vec(vocab_size, Config.EMBEDDING_DIM)
    
    print(f"æ¨¡å‹å‚æ•°:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print(f"  åµŒå…¥ç»´åº¦: {Config.EMBEDDING_DIM}")
    print(f"  è®¾å¤‡: {Config.DEVICE}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(model)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(user_sequences, resume_from_checkpoint=resume)
    
    return model, trainer

def train_node2vec_model(user_sequences, url_mappings, resume=False):
    """
    è®­ç»ƒ Node2Vec æ¨¡å‹
    """
    print("="*50)
    print("å¼€å§‹ Node2Vec æ¨¡å‹è®­ç»ƒ")
    print("="*50)

    # 1. ä»ç”¨æˆ·åºåˆ—æ„å»ºå›¾
    # Node2Vec é€šå¸¸åœ¨æ— å‘å›¾ä¸Šæ•ˆæœæ›´å¥½ï¼Œæƒé‡å¯ä»¥æ¥è‡ªå…±ç°é¢‘ç‡
    item_graph = build_graph_from_sequences(user_sequences, directed=False)
    if not item_graph:
        print("é”™è¯¯:æœªèƒ½ä»ç”¨æˆ·åºåˆ—æ„å»ºå›¾ã€‚è¯·æ£€æŸ¥æ•°æ®ã€‚")
        return None, None

    # 2. ç”Ÿæˆéšæœºæ¸¸èµ°
    print("ç”Ÿæˆ Node2Vec éšæœºæ¸¸èµ°...")
    
    # ä½¿ç”¨å¸¦ç¼“å­˜çš„éšæœºæ¸¸èµ°ç”Ÿæˆå™¨
    node2vec_walks = generate_node2vec_walks_with_cache(
        graph=item_graph,
        num_walks=Config.NUM_WALKS,
        walk_length=Config.WALK_LENGTH,
        p=Config.P_PARAM,
        q=Config.Q_PARAM,
        use_cache=Config.USE_WALKS_CACHE,
        force_regenerate=Config.FORCE_REGENERATE_WALKS
    )
    if not node2vec_walks:
        print("é”™è¯¯:æœªèƒ½ç”Ÿæˆ Node2Vec éšæœºæ¸¸èµ°ã€‚")
        return None, None
    
    print(f"å·²ç”Ÿæˆ {len(node2vec_walks)} æ¡éšæœºæ¸¸èµ°ã€‚")

    # 3. åˆ›å»º Node2Vec æ¨¡å‹
    vocab_size = len(url_mappings['url_to_id']) # è¯æ±‡è¡¨å¤§å°ä¸Item2Vecç›¸åŒ
    model = Node2Vec(vocab_size, Config.EMBEDDING_DIM)
    
    print(f"Node2Vec æ¨¡å‹å‚æ•°:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print(f"  åµŒå…¥ç»´åº¦: {Config.EMBEDDING_DIM}")
    print(f"  På‚æ•°: {Config.P_PARAM}")
    print(f"  Qå‚æ•°: {Config.Q_PARAM}")
    print(f"  æ¸¸èµ°é•¿åº¦: {Config.WALK_LENGTH}")
    print(f"  æ¯ä¸ªèŠ‚ç‚¹çš„æ¸¸èµ°æ¬¡æ•°: {Config.NUM_WALKS}")
    print(f"  è®¾å¤‡: {Config.DEVICE}")

    # 4. åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(model) # Traineråº”è¯¥å¯ä»¥å¤ç”¨

    # 5. å¼€å§‹è®­ç»ƒ (ä½¿ç”¨ç”Ÿæˆçš„éšæœºæ¸¸èµ°ä½œä¸ºè¾“å…¥åºåˆ—)
    # æ³¨æ„: Trainer çš„ create_dataloader å’Œ SkipGramDataset éœ€è¦èƒ½å¤Ÿå¤„ç†è¿™äº›æ¸¸èµ°
    # SkipGramDataset æœŸæœ›çš„æ˜¯ä¸€ä¸ªåºåˆ—åˆ—è¡¨ï¼Œè¿™ä¸ node2vec_walks çš„è¾“å‡ºæ ¼å¼ä¸€è‡´
    print("å¼€å§‹ä½¿ç”¨ç”Ÿæˆçš„æ¸¸èµ°è®­ç»ƒ Node2Vec æ¨¡å‹...")
    trainer.train(node2vec_walks, resume_from_checkpoint=resume) # å°†æ¸¸èµ°åºåˆ—ä¼ é€’ç»™è®­ç»ƒå™¨
    
    return model, trainer

def visualize_results(model, user_sequences, url_mappings):
    """
    å¯è§†åŒ–ç»“æœ
    è¿›è¡Œç”¨æˆ·å’Œç‰©å“åµŒå…¥å‘é‡çš„ t-SNE å¯è§†åŒ–
    """
    print("="*50)
    print("å¼€å§‹åµŒå…¥å‘é‡å¯è§†åŒ–")
    print("="*50)
    
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = Visualizer(model, user_sequences, url_mappings)
    
    # å¯è§†åŒ–ç”¨æˆ·åµŒå…¥å‘é‡
    print("\n1. ç”¨æˆ·åµŒå…¥å‘é‡å¯è§†åŒ–")
    visualizer.visualize_user_embeddings(
        sample_size=500,  # é‡‡æ ·500ä¸ªç”¨æˆ·
        perplexity=30,    # t-SNE å›°æƒ‘åº¦
        n_iter=1000       # t-SNE è¿­ä»£æ¬¡æ•°
    )
    
    # å¯è§†åŒ–ç‰©å“åµŒå…¥å‘é‡
    print("\n2. ç‰©å“åµŒå…¥å‘é‡å¯è§†åŒ–")
    visualizer.visualize_item_embeddings(
        sample_size=1000,  # é‡‡æ ·1000ä¸ªç‰©å“
        perplexity=30,     # t-SNE å›°æƒ‘åº¦
        n_iter=1000        # t-SNE è¿­ä»£æ¬¡æ•°
    )
    
    print("="*50)
    print("å¯è§†åŒ–å®Œæˆ")
    print("="*50)

def compute_user_embeddings(model, user_sequences, url_mappings, save_path=None):
    """
    è®¡ç®—å¹¶ä¿å­˜ç”¨æˆ·åµŒå…¥å‘é‡
    """
    print("="*50)
    print("è®¡ç®—ç”¨æˆ·åµŒå…¥å‘é‡")
    print("="*50)
    
    # åˆ›å»ºç”¨æˆ·åµŒå…¥è®¡ç®—å™¨
    user_embedding = UserEmbedding(model, user_sequences, url_mappings)
    
    # è®¡ç®—ç”¨æˆ·åµŒå…¥
    user_embeddings = user_embedding.compute_user_embeddings()
    
    print(f"å·²è®¡ç®— {len(user_embeddings)} ä¸ªç”¨æˆ·çš„åµŒå…¥å‘é‡")
    
    # ä¿å­˜ç”¨æˆ·åµŒå…¥
    if save_path:
        import pickle
        with open(save_path, 'wb') as f:
            pickle.dump(user_embeddings, f)
        print(f"ç”¨æˆ·åµŒå…¥å·²ä¿å­˜åˆ°: {save_path}")
    
    return user_embeddings

def load_trained_model(model_path, vocab_size):
    """
    åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    """
    model = Item2Vec(vocab_size, Config.EMBEDDING_DIM)
    
    if os.path.exists(model_path):
        # Add Config to safe globals for weights_only=True loading
        torch.serialization.add_safe_globals([Config])
        checkpoint = torch.load(model_path, map_location=Config.DEVICE_OBJ, weights_only=True)
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"å·²åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹: {model_path}")
    else:
        print(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        return None
    
    return model

def compute_location_embeddings(location_model, user_base_stations, base_station_mappings, location_processor=None, save_path=None):
    """
    è®¡ç®—ç”¨æˆ·ä½ç½®åµŒå…¥
    
    Args:
        location_model: è®­ç»ƒå¥½çš„ä½ç½®åµŒå…¥æ¨¡å‹
        user_base_stations: ç”¨æˆ·åŸºç«™è¿æ¥æ•°æ® {user_id: {'base_stations': [...], 'weights': [...], 'total_duration': ...}}
        base_station_mappings: åŸºç«™æ˜ å°„å­—å…¸
        location_processor: ä½ç½®æ•°æ®å¤„ç†å™¨ï¼ˆç”¨äºç‰¹å¾å¤„ç†ï¼‰
        save_path: ä¿å­˜è·¯å¾„
    
    Returns:
        ç”¨æˆ·ä½ç½®åµŒå…¥å­—å…¸
    """
    if not location_model or not user_base_stations:
        return {}
    
    print("å¼€å§‹è®¡ç®—ç”¨æˆ·ä½ç½®åµŒå…¥...")
    
    # è·å–åŸºç«™åµŒå…¥
    base_station_embeddings = {}
    
    # å¯¹äºPyTorchæ¨¡å‹ï¼Œç›´æ¥ä»æ¨¡å‹æƒé‡è·å–åµŒå…¥
    for bs_id in base_station_mappings['base_station_to_id'].keys():
        idx = base_station_mappings['base_station_to_id'][bs_id]
        if hasattr(location_model, 'in_embeddings'):
            # Item2Vecå’ŒNode2Vecä½¿ç”¨in_embeddings
            embedding = location_model.in_embeddings.weight[idx].detach()
            base_station_embeddings[bs_id] = embedding
        elif hasattr(location_model, 'embeddings'):
            # å…¶ä»–æ¨¡å‹å¯èƒ½ä½¿ç”¨embeddings
            embedding = location_model.embeddings.weight[idx].detach()
            base_station_embeddings[bs_id] = embedding
        else:
            # å¦‚æœæ¨¡å‹ç»“æ„ä¸åŒï¼Œä½¿ç”¨å…¶ä»–æ–¹æ³•è·å–åµŒå…¥
            # è¿™é‡Œå¯ä»¥æ ¹æ®å…·ä½“æ¨¡å‹ç»“æ„è°ƒæ•´
            print(f"è­¦å‘Šï¼šæ— æ³•ä»æ¨¡å‹ä¸­è·å–åŸºç«™ {bs_id} çš„åµŒå…¥")
            continue
    
    # åˆ›å»ºç”¨æˆ·ä½ç½®åµŒå…¥è®¡ç®—å™¨
    location_embedding_calculator = UserLocationEmbedding(
        Config, base_station_embeddings, location_processor
    )
    
    # è®¡ç®—ç”¨æˆ·ä½ç½®åµŒå…¥
    user_location_embeddings = {}
    for user_id, data in user_base_stations.items():
        base_stations = data['base_stations']
        weights = data['weights']
        
        # è®¡ç®—ä½ç½®åµŒå…¥
        location_embedding = location_embedding_calculator(base_stations, weights)
        user_location_embeddings[user_id] = location_embedding.detach().numpy()
    
    print(f"å®Œæˆè®¡ç®— {len(user_location_embeddings)} ä¸ªç”¨æˆ·çš„ä½ç½®åµŒå…¥")
    
    # ä¿å­˜ç»“æœ
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        np.save(save_path, user_location_embeddings)
        print(f"ä½ç½®åµŒå…¥å·²ä¿å­˜åˆ°: {save_path}")
    
    return user_location_embeddings

def train_attribute_models(behavior_model, user_sequences, user_attributes, attribute_info, url_mappings):
    """
    è®­ç»ƒå±æ€§æ¨¡å‹
    """
    if not Config.ENABLE_ATTRIBUTES or user_attributes is None or attribute_info is None:
        print("å±æ€§è®­ç»ƒæœªå¯ç”¨æˆ–å±æ€§æ•°æ®ä¸å¯ç”¨ï¼Œè·³è¿‡å±æ€§è®­ç»ƒ")
        return None, None
    
    print("="*50)
    print("å¼€å§‹å±æ€§æ¨¡å‹è®­ç»ƒ")
    print("="*50)
    
    # åˆ›å»ºå±æ€§è®­ç»ƒå™¨
    attribute_trainer = AttributeTrainer(
        behavior_model, user_sequences, user_attributes, 
        attribute_info, url_mappings, Config
    )
    
    # å¼€å§‹è®­ç»ƒ
    attribute_trainer.train()
    
    return attribute_trainer.attribute_model, attribute_trainer.fusion_model

def compute_enhanced_user_embeddings(behavior_model, attribute_model, fusion_model, 
                                   user_sequences, user_attributes, url_mappings, attribute_info, 
                                   location_model=None, user_location_sequences=None, 
                                   base_station_mappings=None, location_weights=None, 
                                   location_processor=None, save_path=None):
    """
    è®¡ç®—å¢å¼ºçš„ç”¨æˆ·åµŒå…¥å‘é‡ï¼ˆè¡Œä¸º+å±æ€§+ä½ç½®ï¼‰
    """
    if not Config.ENABLE_ATTRIBUTES or attribute_model is None or fusion_model is None:
        print("å±æ€§æ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸºç¡€ç”¨æˆ·åµŒå…¥")
        return compute_user_embeddings(behavior_model, user_sequences, url_mappings, save_path)
    
    print("="*50)
    print("è®¡ç®—å¢å¼ºç”¨æˆ·åµŒå…¥å‘é‡ï¼ˆè¡Œä¸º+å±æ€§+ä½ç½®ï¼‰")
    print("="*50)
    
    # åˆ›å»ºå¢å¼ºç”¨æˆ·åµŒå…¥è®¡ç®—å™¨
    enhanced_user_embedding = EnhancedUserEmbedding(
        behavior_model=behavior_model,
        attribute_model=attribute_model,
        fusion_model=fusion_model,
        user_sequences=user_sequences,
        user_attributes=user_attributes,
        url_mappings=url_mappings,
        attribute_info=attribute_info,
        location_model=location_model,
        user_location_sequences=user_location_sequences,
        base_station_mappings=base_station_mappings,
        location_weights=location_weights,
        location_processor=location_processor
    )
    
    # è®¡ç®—å¢å¼ºåµŒå…¥
    enhanced_embeddings = enhanced_user_embedding.compute_enhanced_user_embeddings()
    
    print(f"å·²è®¡ç®— {len(enhanced_embeddings)} ä¸ªç”¨æˆ·çš„å¢å¼ºåµŒå…¥å‘é‡")
    
    # ä¿å­˜å¢å¼ºåµŒå…¥
    if save_path:
        import pickle
        with open(save_path, 'wb') as f:
            pickle.dump(enhanced_embeddings, f)
        print(f"å¢å¼ºç”¨æˆ·åµŒå…¥å·²ä¿å­˜åˆ°: {save_path}")
    
    return enhanced_embeddings

def compute_new_user_embeddings(behavior_model, attribute_model, fusion_model,
                              url_mappings, attribute_info, base_station_mappings=None,
                              location_model=None, location_processor=None,
                              new_user_behavior_path=None, new_user_attribute_path=None, 
                              new_user_location_path=None, save_path=None):
    """
    ä¸ºæ–°ç”¨æˆ·è®¡ç®—å‘é‡è¡¨ç¤º
    
    Args:
        behavior_model: è®­ç»ƒå¥½çš„è¡Œä¸ºæ¨¡å‹
        attribute_model: è®­ç»ƒå¥½çš„å±æ€§æ¨¡å‹
        fusion_model: è®­ç»ƒå¥½çš„èåˆæ¨¡å‹
        url_mappings: URLæ˜ å°„å­—å…¸
        attribute_info: å±æ€§ä¿¡æ¯å­—å…¸
        base_station_mappings: åŸºç«™æ˜ å°„å­—å…¸
        location_model: è®­ç»ƒå¥½çš„ä½ç½®æ¨¡å‹
        location_processor: ä½ç½®æ•°æ®å¤„ç†å™¨
        new_user_behavior_path: æ–°ç”¨æˆ·è¡Œä¸ºæ•°æ®è·¯å¾„
        new_user_attribute_path: æ–°ç”¨æˆ·å±æ€§æ•°æ®è·¯å¾„
        new_user_location_path: æ–°ç”¨æˆ·ä½ç½®æ•°æ®è·¯å¾„
        save_path: ä¿å­˜è·¯å¾„
    
    Returns:
        æ–°ç”¨æˆ·å‘é‡å­—å…¸
    """
    print("="*50)
    print("è®¡ç®—æ–°ç”¨æˆ·å‘é‡è¡¨ç¤º")
    print("="*50)
    
    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤è·¯å¾„
    if new_user_behavior_path is None:
        new_user_behavior_path = Config.NEW_USER_BEHAVIOR_PATH
    if new_user_attribute_path is None:
        new_user_attribute_path = Config.NEW_USER_ATTRIBUTE_PATH
    if new_user_location_path is None:
        new_user_location_path = Config.NEW_USER_LOCATION_PATH
    
    # åŠ è½½æ–°ç”¨æˆ·æ•°æ®
    new_user_data = load_new_user_data(
        behavior_path=new_user_behavior_path,
        attribute_path=new_user_attribute_path,
        location_path=new_user_location_path,
        url_mappings=url_mappings,
        attribute_info=attribute_info,
        base_station_mappings=base_station_mappings,
        location_processor=location_processor
    )
    
    if not new_user_data:
        print("æ²¡æœ‰æ‰¾åˆ°æ–°ç”¨æˆ·æ•°æ®")
        return {}
    
    new_user_sequences = new_user_data['user_sequences']
    new_user_attributes = new_user_data['user_attributes']
    new_user_location_data = new_user_data['user_location_data']
    
    print(f"åŠ è½½äº† {len(new_user_sequences)} ä¸ªæ–°ç”¨æˆ·çš„è¡Œä¸ºæ•°æ®")
    print(f"åŠ è½½äº† {len(new_user_attributes)} ä¸ªæ–°ç”¨æˆ·çš„å±æ€§æ•°æ®")
    location_user_count = len(new_user_location_data.get('user_location_sequences', {}))
    print(f"åŠ è½½äº† {location_user_count} ä¸ªæ–°ç”¨æˆ·çš„ä½ç½®æ•°æ®")
    
    # è®¡ç®—æ–°ç”¨æˆ·å‘é‡
    if (Config.ENABLE_ATTRIBUTES and attribute_model is not None and fusion_model is not None 
        and len(new_user_attributes) > 0):
        # ä½¿ç”¨å¢å¼ºåµŒå…¥ï¼ˆè¡Œä¸º+å±æ€§+ä½ç½®ï¼‰
        new_user_embeddings = compute_enhanced_user_embeddings(
            behavior_model=behavior_model,
            attribute_model=attribute_model,
            fusion_model=fusion_model,
            user_sequences=new_user_sequences,
            user_attributes=new_user_attributes,
            url_mappings=url_mappings,
            attribute_info=attribute_info,
            location_model=location_model,
            user_location_sequences=new_user_location_data.get('user_location_sequences'),
            base_station_mappings=base_station_mappings,
            location_weights=new_user_location_data.get('location_weights'),
            location_processor=location_processor,
            save_path=None  # æš‚æ—¶ä¸ä¿å­˜ï¼Œæœ€åç»Ÿä¸€ä¿å­˜
        )
    else:
        # ä»…ä½¿ç”¨è¡Œä¸ºåµŒå…¥
        new_user_embeddings = compute_user_embeddings(
            model=behavior_model,
            user_sequences=new_user_sequences,
            url_mappings=url_mappings,
            save_path=None
        )
    
    print(f"æˆåŠŸè®¡ç®— {len(new_user_embeddings)} ä¸ªæ–°ç”¨æˆ·çš„å‘é‡è¡¨ç¤º")
    
    # ä¿å­˜ç»“æœ
    if save_path:
        import pickle
        with open(save_path, 'wb') as f:
            pickle.dump(new_user_embeddings, f)
        print(f"æ–°ç”¨æˆ·å‘é‡å·²ä¿å­˜åˆ°: {save_path}")
    
    return new_user_embeddings

def load_new_user_data(behavior_path, attribute_path, location_path,
                      url_mappings, attribute_info, base_station_mappings=None, 
                      location_processor=None):
    """
    åŠ è½½æ–°ç”¨æˆ·çš„æ‰€æœ‰æ•°æ®ï¼Œå¹¶æ ¹æ®è®­ç»ƒå®ä½“è¿›è¡Œè¿‡æ»¤
    
    Args:
        behavior_path: æ–°ç”¨æˆ·è¡Œä¸ºæ•°æ®è·¯å¾„
        attribute_path: æ–°ç”¨æˆ·å±æ€§æ•°æ®è·¯å¾„
        location_path: æ–°ç”¨æˆ·ä½ç½®æ•°æ®è·¯å¾„
        url_mappings: URLæ˜ å°„å­—å…¸
        attribute_info: å±æ€§ä¿¡æ¯å­—å…¸
        base_station_mappings: åŸºç«™æ˜ å°„å­—å…¸
        location_processor: ä½ç½®æ•°æ®å¤„ç†å™¨
    
    Returns:
        åŒ…å«æ‰€æœ‰æ–°ç”¨æˆ·æ•°æ®çš„å­—å…¸
    """
    result = {
        'user_sequences': {},
        'user_attributes': {},
        'user_location_data': {}
    }
    
    # åŠ è½½è®­ç»ƒå®ä½“è®°å½•ç”¨äºè¿‡æ»¤
    training_entities = load_training_entities(Config.PROCESSED_DATA_PATH)
    if training_entities is None:
        print("è­¦å‘Šï¼šæ— æ³•åŠ è½½è®­ç»ƒå®ä½“è®°å½•ï¼Œå°†å°è¯•ä½¿ç”¨æ‰€æœ‰æ–°ç”¨æˆ·æ•°æ®ï¼ˆå¯èƒ½å¯¼è‡´é”™è¯¯ï¼‰")
    else:
        print(f"å·²åŠ è½½è®­ç»ƒå®ä½“è®°å½•: {len(training_entities['urls'])} ä¸ªURL")
        if 'base_stations' in training_entities:
            print(f"  {len(training_entities['base_stations'])} ä¸ªåŸºç«™")
    
    # åˆå§‹åŒ–è¿‡æ»¤ç»Ÿè®¡å˜é‡
    unknown_urls = set()
    unknown_base_stations = set()
    
    # 1. åŠ è½½æ–°ç”¨æˆ·è¡Œä¸ºæ•°æ®
    if os.path.exists(behavior_path):
        print(f"åŠ è½½æ–°ç”¨æˆ·è¡Œä¸ºæ•°æ®: {behavior_path}")
        try:
            # ä½¿ç”¨ç°æœ‰çš„æ•°æ®é¢„å¤„ç†å™¨
            preprocessor = DataPreprocessor(Config)
            
            # ç›´æ¥åŠ è½½å¹¶å¤„ç†è¡Œä¸ºæ•°æ®
            df = pd.read_csv(behavior_path, sep='\t')
            print(f"æ–°ç”¨æˆ·è¡Œä¸ºæ•°æ®å½¢çŠ¶: {df.shape}")
            
            # å¤„ç†è¡Œä¸ºåºåˆ—ï¼Œæ ¹æ®è®­ç»ƒå®ä½“è®°å½•è¿›è¡Œè¿‡æ»¤
            user_sequences = {}
            url_to_id = url_mappings['url_to_id']
            
            # ç»Ÿè®¡è¿‡æ»¤ä¿¡æ¯
            total_records = len(df)
            filtered_records = 0
            
            # æŒ‰ç”¨æˆ·åˆ†ç»„å¤„ç†
            for user_id, group in df.groupby('user_id'):
                sequence = []
                for _, row in group.iterrows():
                    url = row['url']
                    
                    # æ£€æŸ¥URLæ˜¯å¦åœ¨è®­ç»ƒå®ä½“è®°å½•ä¸­
                    if training_entities and url not in training_entities['urls']:
                        unknown_urls.add(url)
                        filtered_records += 1
                        continue
                    
                    if url in url_to_id:  # åªå¤„ç†è®­ç»ƒæ—¶è§è¿‡çš„URL
                        sequence.append(url_to_id[url])
                    else:
                        filtered_records += 1
                
                if sequence:  # åªä¿ç•™æœ‰æœ‰æ•ˆURLçš„ç”¨æˆ·
                    user_sequences[user_id] = sequence
            
            result['user_sequences'] = user_sequences
            
            # è¾“å‡ºè¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯
            print(f"æˆåŠŸå¤„ç† {len(user_sequences)} ä¸ªæ–°ç”¨æˆ·çš„è¡Œä¸ºåºåˆ—")
            if filtered_records > 0:
                print(f"  è¿‡æ»¤äº† {filtered_records}/{total_records} æ¡è®°å½•ï¼ˆURLä¸åœ¨è®­ç»ƒæ•°æ®ä¸­ï¼‰")
                if unknown_urls:
                    print(f"  æœªçŸ¥URLç¤ºä¾‹: {list(unknown_urls)[:5]}{'...' if len(unknown_urls) > 5 else ''}")
            
        except Exception as e:
            print(f"åŠ è½½æ–°ç”¨æˆ·è¡Œä¸ºæ•°æ®æ—¶å‡ºé”™: {e}")
    
    # 2. åŠ è½½æ–°ç”¨æˆ·å±æ€§æ•°æ®
    if Config.ENABLE_ATTRIBUTES and attribute_path and os.path.exists(attribute_path):
        print(f"åŠ è½½æ–°ç”¨æˆ·å±æ€§æ•°æ®: {attribute_path}")
        try:
            # ä½¿ç”¨ç°æœ‰çš„å±æ€§å¤„ç†é€»è¾‘
            preprocessor = DataPreprocessor(Config)
            
            # åŠ è½½å±æ€§æ•°æ®
            attr_df = pd.read_csv(attribute_path, sep='\t')
            print(f"æ–°ç”¨æˆ·å±æ€§æ•°æ®å½¢çŠ¶: {attr_df.shape}")
            
            # å¤„ç†å±æ€§æ•°æ®ï¼Œä½¿ç”¨è®­ç»ƒæ—¶çš„ç¼–ç å™¨
            user_attributes = {}
            
            for _, row in attr_df.iterrows():
                user_id = row['user_id']
                user_attrs = {}
                
                for attr_name, attr_info_item in attribute_info.items():
                    if attr_name in row:
                        attr_value = row[attr_name]
                        
                        if attr_info_item['type'] == 'categorical':
                            # å¯¹äºç±»åˆ«å±æ€§ï¼Œä½¿ç”¨è®­ç»ƒæ—¶çš„ç¼–ç 
                            # ç”±äºè®­ç»ƒæ•°æ®ä¸­æ‰€æœ‰ç±»åˆ«å±æ€§éƒ½è¢«ç¼–ç ä¸º0ï¼ˆOtherç±»åˆ«ï¼‰ï¼Œ
                            # æ–°ç”¨æˆ·çš„ç±»åˆ«å±æ€§ä¹Ÿä½¿ç”¨0
                            user_attrs[attr_name] = 0
                        else:
                            # æ•°å€¼å±æ€§
                            user_attrs[attr_name] = float(attr_value)
                
                if user_attrs:
                    user_attributes[user_id] = user_attrs
            
            result['user_attributes'] = user_attributes
            print(f"æˆåŠŸå¤„ç† {len(user_attributes)} ä¸ªæ–°ç”¨æˆ·çš„å±æ€§æ•°æ®")
            
        except Exception as e:
            print(f"åŠ è½½æ–°ç”¨æˆ·å±æ€§æ•°æ®æ—¶å‡ºé”™: {e}")
    
    # 3. åŠ è½½æ–°ç”¨æˆ·ä½ç½®æ•°æ®
    if Config.ENABLE_LOCATION and location_path and os.path.exists(location_path) and base_station_mappings:
        print(f"åŠ è½½æ–°ç”¨æˆ·ä½ç½®æ•°æ®: {location_path}")
        try:
            # ä½¿ç”¨ä½ç½®å¤„ç†å™¨
            if location_processor is None:
                location_processor = LocationProcessor(Config)
            
            # åŠ è½½ä½ç½®æ•°æ®
            location_df = pd.read_csv(location_path, sep='\t')
            print(f"æ–°ç”¨æˆ·ä½ç½®æ•°æ®å½¢çŠ¶: {location_df.shape}")
            
            # å¤„ç†ä½ç½®æ•°æ®
            user_location_sequences = {}
            location_weights = {}
            base_station_to_id = base_station_mappings['base_station_to_id']
            
            # ç»Ÿè®¡è¿‡æ»¤ä¿¡æ¯
            total_location_records = len(location_df)
            filtered_location_records = 0
            
            for user_id, group in location_df.groupby('user_id'):
                # è®¡ç®—æ¯ä¸ªåŸºç«™çš„æƒé‡ï¼ˆåŸºäºåœç•™æ—¶é—´ï¼‰
                base_station_durations = group.groupby('base_station_id')['duration'].sum()
                
                # æ ¹æ®è®­ç»ƒå®ä½“è®°å½•è¿‡æ»¤åŸºç«™
                valid_stations = []
                valid_weights = []
                
                for bs_id, duration in base_station_durations.items():
                    # æ£€æŸ¥åŸºç«™æ˜¯å¦åœ¨è®­ç»ƒå®ä½“è®°å½•ä¸­
                    if training_entities and 'base_stations' in training_entities and bs_id not in training_entities['base_stations']:
                        unknown_base_stations.add(bs_id)
                        filtered_location_records += len(group[group['base_station_id'] == bs_id])
                        continue
                    
                    if bs_id in base_station_to_id:
                        valid_stations.append(base_station_to_id[bs_id])
                        valid_weights.append(duration)
                    else:
                        filtered_location_records += len(group[group['base_station_id'] == bs_id])
                
                if len(valid_stations) >= Config.LOCATION_MIN_CONNECTIONS:
                    # ç”Ÿæˆä½ç½®åºåˆ—ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
                    user_location_sequences[user_id] = valid_stations
                    
                    # è®¡ç®—æƒé‡ï¼ˆå½’ä¸€åŒ–ï¼‰
                    total_duration = sum(valid_weights)
                    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨åŸºç«™IDè€Œä¸æ˜¯åŸºç«™åç§°ä½œä¸ºé”®
                    normalized_weights = {base_station_to_id[bs_id]: weight/total_duration 
                                        for bs_id, weight in base_station_durations.items() if bs_id in base_station_to_id}
                    location_weights[user_id] = normalized_weights
            
            result['user_location_data'] = {
                'user_location_sequences': user_location_sequences,
                'location_weights': location_weights
            }
            print(f"æˆåŠŸå¤„ç† {len(user_location_sequences)} ä¸ªæ–°ç”¨æˆ·çš„ä½ç½®æ•°æ®")
            if filtered_location_records > 0:
                print(f"  è¿‡æ»¤äº† {filtered_location_records}/{total_location_records} æ¡ä½ç½®è®°å½•ï¼ˆåŸºç«™ä¸åœ¨è®­ç»ƒæ•°æ®ä¸­ï¼‰")
                if unknown_base_stations:
                    print(f"  æœªçŸ¥åŸºç«™ç¤ºä¾‹: {list(unknown_base_stations)[:5]}{'...' if len(unknown_base_stations) > 5 else ''}")
            
        except Exception as e:
            print(f"åŠ è½½æ–°ç”¨æˆ·ä½ç½®æ•°æ®æ—¶å‡ºé”™: {e}")
    
    # ç”Ÿæˆè¿‡æ»¤æŠ¥å‘Š
    if training_entities:
        # ç”ŸæˆæŠ¥å‘Šä¿å­˜è·¯å¾„
        report_save_path = os.path.join(Config.PROCESSED_DATA_PATH, 'new_user_compatibility_report.json')
        generate_filtering_report(result, training_entities, behavior_path, location_path, 
                                unknown_urls, unknown_base_stations, report_save_path)
    
    return result

def generate_filtering_report(new_user_data, training_entities, behavior_path, location_path, 
                            unknown_urls, unknown_base_stations, save_path=None):
    """
    ç”Ÿæˆæ–°ç”¨æˆ·æ•°æ®è¿‡æ»¤æŠ¥å‘Š
    
    Args:
        new_user_data: æ–°ç”¨æˆ·æ•°æ®å­—å…¸
        training_entities: è®­ç»ƒå®ä½“è®°å½•
        behavior_path: è¡Œä¸ºæ•°æ®è·¯å¾„
        location_path: ä½ç½®æ•°æ®è·¯å¾„
        unknown_urls: æœªçŸ¥URLé›†åˆ
        unknown_base_stations: æœªçŸ¥åŸºç«™é›†åˆ
        save_path: æŠ¥å‘Šä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    print("\n" + "="*50)
    print("ğŸ” æ–°ç”¨æˆ·æ•°æ®è¿‡æ»¤æŠ¥å‘Š")
    print("="*50)
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"ğŸ“Š å¤„ç†ç»“æœç»Ÿè®¡:")
    print(f"  âœ… æˆåŠŸå¤„ç†ç”¨æˆ·æ•°é‡: {len(new_user_data['user_sequences'])}")
    
    # URLè¿‡æ»¤ç»Ÿè®¡
    if unknown_urls:
        print(f"\nğŸŒ URLè¿‡æ»¤ç»Ÿè®¡:")
        print(f"  âŒ æœªçŸ¥URLæ•°é‡: {len(unknown_urls)}")
        print(f"  ğŸ“ è®­ç»ƒURLæ•°é‡: {len(training_entities['urls'])}")
        print(f"  ğŸ“‹ æœªçŸ¥URLåˆ—è¡¨: {sorted(unknown_urls)}")
        
        # å»ºè®®
        print(f"\nğŸ’¡ å»ºè®®:")
        print(f"  â€¢ å¦‚æœè¿™äº›URLå¾ˆé‡è¦ï¼Œè€ƒè™‘å°†å®ƒä»¬æ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­")
        print(f"  â€¢ æˆ–è€…å¯ä»¥å°†å®ƒä»¬æ˜ å°„åˆ°ç›¸ä¼¼çš„å·²çŸ¥URL")
    
    # åŸºç«™è¿‡æ»¤ç»Ÿè®¡
    if unknown_base_stations:
        print(f"\nğŸ“¡ åŸºç«™è¿‡æ»¤ç»Ÿè®¡:")
        print(f"  âŒ æœªçŸ¥åŸºç«™æ•°é‡: {len(unknown_base_stations)}")
        print(f"  ğŸ“ è®­ç»ƒåŸºç«™æ•°é‡: {len(training_entities['base_stations'])}")
        print(f"  ğŸ“‹ æœªçŸ¥åŸºç«™åˆ—è¡¨: {sorted(unknown_base_stations)}")
        
        # å»ºè®®
        print(f"\nğŸ’¡ å»ºè®®:")
        print(f"  â€¢ å¦‚æœè¿™äº›åŸºç«™å¾ˆé‡è¦ï¼Œè€ƒè™‘å°†å®ƒä»¬æ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­")
        print(f"  â€¢ æˆ–è€…æ£€æŸ¥åŸºç«™IDçš„å‘½åè§„èŒƒæ˜¯å¦ä¸€è‡´")
    
    # æ•°æ®è´¨é‡è¯„ä¼°
    total_users = len(new_user_data['user_sequences'])
    if total_users > 0:
        print(f"\nğŸ“ˆ æ•°æ®è´¨é‡è¯„ä¼°:")
        
        # è¡Œä¸ºæ•°æ®è´¨é‡
        avg_behavior_length = sum(len(seq) for seq in new_user_data['user_sequences'].values()) / total_users
        print(f"  ğŸ“± å¹³å‡è¡Œä¸ºåºåˆ—é•¿åº¦: {avg_behavior_length:.1f}")
        
        # ä½ç½®æ•°æ®è´¨é‡
        if 'user_location_data' in new_user_data and new_user_data['user_location_data']:
            location_data = new_user_data['user_location_data']
            if 'user_location_sequences' in location_data:
                location_users = len(location_data['user_location_sequences'])
                print(f"  ğŸ“ æœ‰ä½ç½®æ•°æ®çš„ç”¨æˆ·æ¯”ä¾‹: {location_users/total_users*100:.1f}%")
        
        # å±æ€§æ•°æ®è´¨é‡
        if 'user_attributes' in new_user_data:
            attr_users = len(new_user_data['user_attributes'])
            print(f"  ğŸ‘¤ æœ‰å±æ€§æ•°æ®çš„ç”¨æˆ·æ¯”ä¾‹: {attr_users/total_users*100:.1f}%")
    
    print("="*50)
    
    # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    if save_path:
        save_compatibility_report_to_file(new_user_data, training_entities, behavior_path, location_path,
                                         unknown_urls, unknown_base_stations, save_path)

def save_compatibility_report_to_file(new_user_data, training_entities, behavior_path, location_path,
                                     unknown_urls, unknown_base_stations, save_path):
    """
    å°†å…¼å®¹æ€§æŠ¥å‘Šä¿å­˜åˆ°æ–‡ä»¶
    """
    import json
    from datetime import datetime
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_users = len(new_user_data['user_sequences'])
    
    # è¡Œä¸ºæ•°æ®ç»Ÿè®¡
    behavior_stats = None
    if unknown_urls is not None:
        behavior_records = 0
        filtered_behavior_records = 0
        try:
            if os.path.exists(behavior_path):
                import pandas as pd
                df = pd.read_csv(behavior_path, sep='\t')
                behavior_records = len(df)
                filtered_behavior_records = len(df[~df['url'].isin(training_entities['urls'])])
        except:
            pass
            
        behavior_stats = {
            'total_records': behavior_records,
            'filtered_records': filtered_behavior_records,
            'unknown_urls_count': len(unknown_urls),
            'unknown_urls': sorted(list(unknown_urls)),
            'known_urls_count': len(training_entities['urls']),
            'coverage': (len(training_entities['urls']) - len(unknown_urls)) / len(training_entities['urls']) if training_entities['urls'] else 0
        }
    
    # ä½ç½®æ•°æ®ç»Ÿè®¡
    location_stats = None
    if unknown_base_stations is not None and 'base_stations' in training_entities:
        location_records = 0
        filtered_location_records = 0
        try:
            if os.path.exists(location_path):
                import pandas as pd
                df = pd.read_csv(location_path, sep='\t')
                location_records = len(df)
                filtered_location_records = len(df[~df['base_station_id'].isin(training_entities['base_stations'])])
        except:
            pass
            
        location_stats = {
            'total_records': location_records,
            'filtered_records': filtered_location_records,
            'unknown_base_stations_count': len(unknown_base_stations),
            'unknown_base_stations': sorted(list(unknown_base_stations)),
            'known_base_stations_count': len(training_entities['base_stations']),
            'coverage': (len(training_entities['base_stations']) - len(unknown_base_stations)) / len(training_entities['base_stations']) if training_entities['base_stations'] else 0
        }
    
    # æ•°æ®è´¨é‡è¯„ä¼°
    avg_behavior_length = sum(len(seq) for seq in new_user_data['user_sequences'].values()) / total_users if total_users > 0 else 0
    
    location_users = 0
    if 'user_location_data' in new_user_data and new_user_data['user_location_data']:
        location_data = new_user_data['user_location_data']
        if 'user_location_sequences' in location_data:
            location_users = len(location_data['user_location_sequences'])
    
    attr_users = len(new_user_data.get('user_attributes', {}))
    
    # è®¡ç®—æ€»ä½“å…¼å®¹æ€§è¯„åˆ†
    total_score = 0
    max_score = 0
    if behavior_stats:
        total_score += behavior_stats['coverage'] * 50
        max_score += 50
    if location_stats:
        total_score += location_stats['coverage'] * 50
        max_score += 50
    
    final_score = total_score / max_score if max_score > 0 else 1
    
    # ç”ŸæˆæŠ¥å‘Šæ•°æ®
    report = {
        'timestamp': datetime.now().isoformat(),
        'experiment_info': {
            'behavior_data_path': behavior_path,
            'location_data_path': location_path,
            'training_entities_count': {
                'urls': len(training_entities['urls']),
                'base_stations': len(training_entities.get('base_stations', []))
            }
        },
        'processing_results': {
            'total_users_processed': total_users,
            'avg_behavior_sequence_length': round(avg_behavior_length, 2),
            'users_with_location_data': location_users,
            'users_with_attribute_data': attr_users,
            'location_coverage_percent': round(location_users / total_users * 100, 1) if total_users > 0 else 0,
            'attribute_coverage_percent': round(attr_users / total_users * 100, 1) if total_users > 0 else 0
        },
        'behavior_data_analysis': behavior_stats,
        'location_data_analysis': location_stats,
        'compatibility_assessment': {
            'overall_score_percent': round(final_score * 100, 1),
            'rating': (
                'excellent' if final_score >= 0.8 else
                'good' if final_score >= 0.6 else
                'fair' if final_score >= 0.4 else
                'poor'
            ),
            'recommendations': []
        }
    }
    
    # æ·»åŠ å»ºè®®
    recommendations = []
    if behavior_stats and behavior_stats['unknown_urls_count'] > 0:
        recommendations.append("è€ƒè™‘å°†é‡è¦çš„æœªçŸ¥URLæ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­")
        recommendations.append("æˆ–è€…å°†æœªçŸ¥URLæ˜ å°„åˆ°ç›¸ä¼¼çš„å·²çŸ¥URL")
    
    if location_stats and location_stats['unknown_base_stations_count'] > 0:
        recommendations.append("æ£€æŸ¥åŸºç«™IDçš„å‘½åè§„èŒƒæ˜¯å¦ä¸€è‡´")
        recommendations.append("è€ƒè™‘å°†é‡è¦çš„æœªçŸ¥åŸºç«™æ·»åŠ åˆ°è®­ç»ƒæ•°æ®ä¸­")
    
    report['compatibility_assessment']['recommendations'] = recommendations
    
    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ“„ å…¼å®¹æ€§æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")

def load_training_config(experiment_dir):
    """
    åŠ è½½è®­ç»ƒæ—¶ä¿å­˜çš„é…ç½®ï¼Œç”¨äºæ¨ç†é˜¶æ®µ
    """
    config_path = os.path.join(experiment_dir, 'experiment_config.json')
    if os.path.exists(config_path):
        print(f"åŠ è½½è®­ç»ƒæ—¶çš„é…ç½®: {config_path}")
        with open(config_path, 'r', encoding='utf-8') as f:
            saved_config = json.load(f)
        
        # æ›´æ–°Configç±»çš„å±æ€§
        if 'config' in saved_config:
            training_config = saved_config['config']
            for key, value in training_config.items():
                if hasattr(Config, key):
                    # è·³è¿‡è·¯å¾„ç›¸å…³çš„é…ç½®ï¼Œå› ä¸ºè¿™äº›ä¼šåœ¨initialize_experiment_pathsä¸­è®¾ç½®
                    if key.endswith('_PATH') or key.endswith('_DIR'):
                        continue
                    setattr(Config, key, value)
                    print(f"  æ›´æ–°é…ç½®: {key} = {value}")
        
        # é‡æ–°è®¾ç½®è®¾å¤‡å¯¹è±¡
        Config.DEVICE_OBJ = torch.device(Config.DEVICE)
        
        print("è®­ç»ƒæ—¶é…ç½®åŠ è½½å®Œæˆ")
        return True
    else:
        print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°è®­ç»ƒæ—¶çš„é…ç½®æ–‡ä»¶: {config_path}")
        return False

def main():
    """
    ä¸»å‡½æ•°
    """
    parser = argparse.ArgumentParser(description='Item2Vec/Node2Vecç”¨æˆ·è¡¨ç¤ºå‘é‡è®­ç»ƒé¡¹ç›®')
    parser.add_argument('--mode', type=str, choices=['preprocess', 'train', 'visualize', 'compute_embeddings', 'compute_new_users', 'all'], 
                       default='all', help='è¿è¡Œæ¨¡å¼ (preprocess, train, visualize, compute_embeddings, compute_new_users, all)')
    parser.add_argument('--data_path', type=str, help='åŸå§‹æ•°æ®è·¯å¾„ (ä¾‹å¦‚: data/user_behavior.csv)')
    parser.add_argument('--model_path', type=str, help='å·²è®­ç»ƒæ¨¡å‹çš„è·¯å¾„ (ç”¨äºvisualize/compute_embeddingsæ¨¡å¼)')
    parser.add_argument('--resume', action='store_true', help='ä»æœ€æ–°çš„æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    parser.add_argument('--no_train', action='store_true', help='è·³è¿‡è®­ç»ƒï¼Œç›´æ¥ä½¿ç”¨å·²æœ‰æ¨¡å‹ (ä¸visualize/compute_embeddingsæ¨¡å¼ç»“åˆ)')
    parser.add_argument('--experiment_name', type=str, help='è‡ªå®šä¹‰å®éªŒåç§° (é»˜è®¤ä¸ºconfig.pyä¸­çš„EXPERIMENT_NAME)')
    parser.add_argument('--no_cache', action='store_true', help='ç¦ç”¨éšæœºæ¸¸èµ°ç¼“å­˜')
    parser.add_argument('--force_regenerate', action='store_true', help='å¼ºåˆ¶é‡æ–°ç”Ÿæˆéšæœºæ¸¸èµ°ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰')
    parser.add_argument('--enable_attributes', action='store_true', help='å¯ç”¨å±æ€§å‘é‡è®­ç»ƒ')
    parser.add_argument('--attribute_data_path', type=str, help='ç”¨æˆ·å±æ€§æ•°æ®æ–‡ä»¶è·¯å¾„')
    
    # æ–°ç”¨æˆ·å‘é‡è®¡ç®—ç›¸å…³å‚æ•°
    parser.add_argument('--new_user_behavior_path', type=str, help='æ–°ç”¨æˆ·è¡Œä¸ºæ•°æ®è·¯å¾„')
    parser.add_argument('--new_user_attribute_path', type=str, help='æ–°ç”¨æˆ·å±æ€§æ•°æ®è·¯å¾„')
    parser.add_argument('--new_user_location_path', type=str, help='æ–°ç”¨æˆ·ä½ç½®æ•°æ®è·¯å¾„')
    
    args = parser.parse_args()
    
    # 1. åˆå§‹åŒ–å®éªŒè·¯å¾„ (è¿™æ˜¯å…³é”®æ”¹åŠ¨)
    initialize_experiment_paths(experiment_name_override=args.experiment_name, mode=args.mode)
    
    # 2. å¯¹äºæ¨ç†ç›¸å…³æ¨¡å¼ï¼ŒåŠ è½½è®­ç»ƒæ—¶çš„é…ç½®
    if args.mode in ['compute_new_users', 'visualize', 'compute_embeddings']:
        experiment_dir = get_experiment_dir(Config.EXPERIMENT_NAME)
        load_training_config(experiment_dir)
    
    # 3. è®¾ç½®éšæœºç§å­
    set_random_seed(Config.RANDOM_SEED)
    
    # 4. åˆ›å»ºç›®å½• (ç°åœ¨å®ƒä¼šä½¿ç”¨initialize_experiment_pathsç¡®å®šçš„è·¯å¾„)
    create_directories()
    
    # æ›´æ–° DATA_PATH å¦‚æœé€šè¿‡å‘½ä»¤è¡ŒæŒ‡å®š (é€šå¸¸ç”¨äº preprocess)
    if args.data_path:
        Config.DATA_PATH = args.data_path
        print(f"ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„æ•°æ®è·¯å¾„: {Config.DATA_PATH}")

    # å¤„ç†ç¼“å­˜ç›¸å…³å‚æ•°
    if args.no_cache:
        Config.USE_WALKS_CACHE = False
        print("å·²ç¦ç”¨éšæœºæ¸¸èµ°ç¼“å­˜")
    
    if args.force_regenerate:
        Config.FORCE_REGENERATE_WALKS = True
        print("å°†å¼ºåˆ¶é‡æ–°ç”Ÿæˆéšæœºæ¸¸èµ°ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰")

    # å¤„ç†å±æ€§ç›¸å…³å‚æ•°
    if args.enable_attributes:
        Config.ENABLE_ATTRIBUTES = True
        print("å·²å¯ç”¨å±æ€§å‘é‡è®­ç»ƒ")
    
    if args.attribute_data_path:
        Config.ATTRIBUTE_DATA_PATH = args.attribute_data_path
        print(f"ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„å±æ€§æ•°æ®è·¯å¾„: {Config.ATTRIBUTE_DATA_PATH}")

    print(f"\nItem2Vec/Node2Vecç”¨æˆ·è¡¨ç¤ºå‘é‡è®­ç»ƒé¡¹ç›®")
    print(f"å®éªŒåç§° (Config): {Config.EXPERIMENT_NAME}")
    # ä½¿ç”¨ get_experiment_dir() æ¥è·å–ç¼“å­˜çš„/æœ€ç»ˆç¡®å®šçš„è·¯å¾„è¿›è¡Œæ˜¾ç¤º
    print(f"å®é™…å®éªŒç›®å½•: {get_experiment_dir(Config.EXPERIMENT_NAME)}") 
    print(f"è¿è¡Œæ¨¡å¼: {args.mode}")
    print(f"è®¾å¤‡: {Config.DEVICE_OBJ}") # ä½¿ç”¨DEVICE_OBJ
    print(f"æ¨¡å‹ç±»å‹ (æ¥è‡ªConfig): {Config.MODEL_TYPE}")
    if Config.MODEL_TYPE == "node2vec":
        print(f"Node2Vec ç¼“å­˜: {'å¯ç”¨' if Config.USE_WALKS_CACHE else 'ç¦ç”¨'}")
        if Config.USE_WALKS_CACHE and Config.FORCE_REGENERATE_WALKS:
            print("ç¼“å­˜æ¨¡å¼: å¼ºåˆ¶é‡æ–°ç”Ÿæˆ")
    print("="*50)
    
    user_sequences = None
    url_mappings = None
    user_attributes = None
    attribute_info = None
    user_location_sequences = None
    base_station_mappings = None
    location_weights = None
    model = None
    location_model = None
    attribute_model = None
    fusion_model = None
    location_processor = None
    
    # æ•°æ®é¢„å¤„ç†
    if args.mode in ['preprocess', 'all']:
        user_sequences, url_mappings, user_attributes, attribute_info, user_location_sequences, base_station_mappings, location_weights = preprocess_data(Config.DATA_PATH) # ä½¿ç”¨Config.DATA_PATH
        if user_sequences is None:
            print("æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return
    
    # å¦‚æœä¸æ˜¯é¢„å¤„ç†æ¨¡å¼ï¼Œéœ€è¦åŠ è½½å·²å¤„ç†çš„æ•°æ®
    # ç¡®ä¿ä»æ­£ç¡®çš„PROCESSED_DATA_PATHåŠ è½½
    if args.mode not in ['preprocess'] and user_sequences is None:
        print(f"å°è¯•ä» {Config.PROCESSED_DATA_PATH} åŠ è½½å·²å¤„ç†æ•°æ®...")
        user_sequences, url_mappings, user_attributes, attribute_info, user_location_sequences, base_station_mappings, location_weights = preprocess_data() # preprocessorå†…éƒ¨ä¼šä½¿ç”¨Config.PROCESSED_DATA_PATH
        if user_sequences is None:
            print("æ— æ³•åŠ è½½æ•°æ®ï¼Œè¯·ç¡®ä¿å·²è¿è¡Œé¢„å¤„ç†æˆ–æä¾›äº†æ­£ç¡®çš„æ•°æ®è·¯å¾„ã€‚ç¨‹åºé€€å‡º")
            return
    
    # æ¨¡å‹è®­ç»ƒ
    if args.mode in ['train', 'all'] and not args.no_train:
        if user_sequences is None or url_mappings is None:
            print("æ•°æ®æœªåŠ è½½ï¼Œæ— æ³•å¼€å§‹è®­ç»ƒã€‚è¯·å…ˆè¿è¡Œ preprocess æ¨¡å¼ã€‚")
            return
        
        if Config.MODEL_TYPE == 'item2vec':
            model, trainer = train_model(user_sequences, url_mappings, args.resume)
        elif Config.MODEL_TYPE == 'node2vec':
            model, trainer = train_node2vec_model(user_sequences, url_mappings, args.resume)
            if model is None: # å¦‚æœNode2Vecè®­ç»ƒä¸­é€”å¤±è´¥ï¼ˆä¾‹å¦‚å›¾æ„å»ºæˆ–æ¸¸èµ°ç”Ÿæˆå¤±è´¥ï¼‰
                print("Node2Vecæ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œç¨‹åºé€€å‡ºã€‚")
                return
        else:
            print(f"æœªçŸ¥çš„æ¨¡å‹ç±»å‹ (æ¥è‡ªConfig): {Config.MODEL_TYPE}")
            return
        
        # è¡Œä¸ºæ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œè®­ç»ƒä½ç½®æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if Config.ENABLE_LOCATION and model is not None:
            location_processor = LocationProcessor(Config)
            
            # åŠ è½½åŸºç«™ç‰¹å¾æ•°æ®
            location_processor.load_base_station_features(Config.LOCATION_FEATURES_PATH)
            
            # è®­ç»ƒä½ç½®æ¨¡å‹
            location_model, base_station_mappings = train_location_model(
                Config, location_processor
            )
            
            # è®¡ç®—ç”¨æˆ·ä½ç½®åµŒå…¥
            if location_model is not None:
                user_base_stations = location_processor.process_user_base_stations(Config.LOCATION_DATA_PATH)
                user_location_embeddings = compute_location_embeddings(
                    location_model, user_base_stations, base_station_mappings, location_processor
                )
            else:
                user_location_embeddings = {}
        else:
            location_processor = None
            user_location_embeddings = {}
            user_base_stations = {}
        
        # è¡Œä¸ºæ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œè®­ç»ƒå±æ€§æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if Config.ENABLE_ATTRIBUTES and model is not None:
            # ä¸ºå±æ€§æ¨¡å‹åˆ›å»ºæ”¯æŒä½ç½®çš„èåˆæ¨¡å‹
            behavior_dim = Config.EMBEDDING_DIM
            attribute_dim = Config.ATTRIBUTE_EMBEDDING_DIM
            location_dim = Config.LOCATION_EMBEDDING_DIM if Config.ENABLE_LOCATION else None
            
            # åˆ›å»ºæ”¯æŒå¤šæ¨¡æ€çš„èåˆæ¨¡å‹
            fusion_model = UserFusionModel(
                behavior_dim, attribute_dim, location_dim, Config
            )
            
            # ä½¿ç”¨ç°æœ‰çš„å±æ€§è®­ç»ƒå™¨ï¼Œä½†ä¼ å…¥ä½ç½®ä¿¡æ¯
            attribute_trainer = AttributeTrainer(
                behavior_model=model,
                user_sequences=user_sequences,
                user_attributes=user_attributes,
                attribute_info=attribute_info,
                url_mappings=url_mappings,
                config=Config
            )
            
            # å¦‚æœæœ‰ä½ç½®ä¿¡æ¯ï¼Œå°†å…¶ä¼ é€’ç»™è®­ç»ƒå™¨
            if location_model is not None and user_location_embeddings:
                attribute_trainer.location_model = location_model
                attribute_trainer.user_location_sequences = user_base_stations
                attribute_trainer.base_station_mappings = base_station_mappings
                attribute_trainer.location_weights = user_location_embeddings
                attribute_trainer.location_processor = location_processor
                attribute_trainer.user_base_stations = user_base_stations
            
            # è®¾ç½®å¤šæ¨¡æ€èåˆæ¨¡å‹
            attribute_trainer.fusion_model = fusion_model
            
            # å¼€å§‹è®­ç»ƒ
            attribute_trainer.train()
            
            attribute_model = attribute_trainer.attribute_model
            fusion_model = attribute_trainer.fusion_model
    
    # åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹ (å¦‚æœéœ€è¦)
    # ç¡®ä¿ä»æ­£ç¡®çš„MODEL_SAVE_PATHåŠ è½½
    if (args.no_train and args.mode in ['train', 'all']) or args.mode in ['visualize', 'compute_embeddings']:
        if model is None: # é¿å…é‡å¤åŠ è½½
            model_save_subdir = Config.MODEL_SAVE_PATH # è·¯å¾„ç°åœ¨ç”±ConfigåŠ¨æ€ç¡®å®š
            
            # æ ¹æ®æ¨¡å‹ç±»å‹ç¡®å®šæ¨¡å‹æ–‡ä»¶å
            model_filename = 'item2vec_model.pth' if Config.MODEL_TYPE == 'item2vec' else 'node2vec_model.pth'
            user_embeddings_filename = 'user_embeddings.pkl' if Config.MODEL_TYPE == 'item2vec' else 'user_embeddings_node2vec.pkl'

            model_load_path = args.model_path or os.path.join(model_save_subdir, model_filename)
            
            if url_mappings is None: # è¯„ä¼°ã€å¯è§†åŒ–æˆ–è®¡ç®—åµŒå…¥æ—¶å¯èƒ½éœ€è¦å…ˆåŠ è½½æ˜ å°„
                print("URL mappings æœªåŠ è½½ï¼Œå°è¯•ä»é¢„å¤„ç†æ•°æ®ä¸­è·å–...")
                _, url_mappings_temp, user_attributes_temp, attribute_info_temp = preprocess_data() # å†æ¬¡è°ƒç”¨ä»¥è·å–æ˜ å°„
                if url_mappings_temp is None:
                    print("æ— æ³•è·å–URL mappingsï¼Œåç»­æ­¥éª¤å¯èƒ½å—é™ã€‚")
                    #å¯ä»¥é€‰æ‹©é€€å‡ºæˆ–ç»§ç»­ï¼Œå–å†³äºåç»­æ­¥éª¤æ˜¯å¦ä¸¥æ ¼éœ€è¦å®ƒ
                else:
                    url_mappings = url_mappings_temp
                    if user_attributes is None:
                        user_attributes = user_attributes_temp
                    if attribute_info is None:
                        attribute_info = attribute_info_temp
            
            if url_mappings is None and args.mode not in ['train']: # å¯¹äºéè®­ç»ƒçš„åç»­æ­¥éª¤ï¼Œè¯æ±‡è¡¨å¤§å°æ˜¯å¿…é¡»çš„
                 print("æ— æ³•ç¡®å®šè¯æ±‡è¡¨å¤§å° (url_mappings is None)ï¼Œæ— æ³•åŠ è½½æ¨¡å‹ã€‚è¯·ç¡®ä¿å·²è¿›è¡Œé¢„å¤„ç†ã€‚")
                 return
            vocab_size = len(url_mappings['url_to_id']) if url_mappings else 0
            if vocab_size == 0 and args.mode not in ['train']: # å†æ¬¡æ£€æŸ¥
                print("è¯æ±‡è¡¨å¤§å°ä¸º0ï¼Œæ— æ³•åŠ è½½æ¨¡å‹ã€‚")
                return
            
            # ä¿®æ”¹: æ ¹æ®æ¨¡å‹ç±»å‹åŠ è½½ä¸åŒçš„æ¨¡å‹
            if Config.MODEL_TYPE == 'item2vec':
                model = load_trained_model(model_load_path, vocab_size) # load_trained_model å†…éƒ¨ä½¿ç”¨ Item2Vec
            elif Config.MODEL_TYPE == 'node2vec':
                # éœ€è¦ä¸€ä¸ªç±»ä¼¼load_trained_modelçš„å‡½æ•°æ¥åŠ è½½Node2Vecï¼Œæˆ–è€…ä¿®æ”¹load_trained_modelä½¿å…¶é€šç”¨
                # æš‚æ—¶ç®€å•å¤„ç†ï¼Œå‡è®¾Node2Vecæ¨¡å‹ä¿å­˜å’ŒItem2Vecä¸€æ ·
                # æ³¨æ„: å®é™…åº”ç”¨ä¸­ï¼Œå¯èƒ½éœ€è¦ç¡®ä¿åŠ è½½çš„æ˜¯æ­£ç¡®çš„æ¨¡å‹ç±»å‹
                temp_node2vec_model = Node2Vec(vocab_size, Config.EMBEDDING_DIM)
                if os.path.exists(model_load_path):
                    torch.serialization.add_safe_globals([Config])
                    checkpoint = torch.load(model_load_path, map_location=Config.DEVICE_OBJ, weights_only=True)
                    temp_node2vec_model.load_state_dict(checkpoint['model_state_dict'])
                    model = temp_node2vec_model
                    print(f"å·²åŠ è½½è®­ç»ƒå¥½çš„Node2Vecæ¨¡å‹: {model_load_path}")
                else:
                    print(f"Node2Vecæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_load_path}")
                    model = None
            else:
                print(f"åŠ è½½æ¨¡å‹æ—¶é‡åˆ°æœªçŸ¥æ¨¡å‹ç±»å‹ (æ¥è‡ªConfig): {Config.MODEL_TYPE}")
                return

            if model is None:
                print(f"æ— æ³•ä» {model_load_path} åŠ è½½æ¨¡å‹ï¼Œç¨‹åºé€€å‡º")
                return
        
        # åŠ è½½å±æ€§æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ä¸”å­˜åœ¨ï¼‰
        if Config.ENABLE_ATTRIBUTES and attribute_model is None and attribute_info is not None:
            attribute_model_path = os.path.join(Config.MODEL_SAVE_PATH, 'best_attribute_models.pth')
            if os.path.exists(attribute_model_path):
                try:
                    attribute_model, fusion_model = load_attribute_models(attribute_model_path, attribute_info, Config)
                    print(f"å·²åŠ è½½å±æ€§æ¨¡å‹: {attribute_model_path}")
                except Exception as e:
                    print(f"åŠ è½½å±æ€§æ¨¡å‹æ—¶å‡ºé”™: {e}")
                    attribute_model = None
                    fusion_model = None
            else:
                print(f"å±æ€§æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {attribute_model_path}")
    
    # ç»“æœå¯è§†åŒ–
    if args.mode in ['visualize', 'all']:
        if model is not None and user_sequences is not None and url_mappings is not None:
            visualize_results(model, user_sequences, url_mappings)
        else:
            print("æ¨¡å‹æˆ–æ•°æ®æœªå‡†å¤‡å¥½ï¼Œè·³è¿‡å¯è§†åŒ–ã€‚è¯·ç¡®ä¿å·²è®­ç»ƒæ¨¡å‹å¹¶åŠ è½½äº†æ•°æ®ã€‚")
    
    # è®¡ç®—ç”¨æˆ·åµŒå…¥ (é€šå¸¸åœ¨'all'æ¨¡å¼æˆ–è€…éœ€è¦æœ€ç»ˆåµŒå…¥æ—¶è¿è¡Œ)
    # ç°åœ¨ä¹Ÿä¸º 'compute_embeddings' æ¨¡å¼å¯ç”¨
    if args.mode in ['all', 'compute_embeddings'] and model is not None and user_sequences is not None and url_mappings is not None:
        if Config.ENABLE_ATTRIBUTES and attribute_model is not None and fusion_model is not None and user_attributes is not None:
            # è®¡ç®—å¢å¼ºç”¨æˆ·åµŒå…¥ï¼ˆæ”¯æŒä½ç½®ä¿¡æ¯ï¼‰
            enhanced_embeddings_filename = f'enhanced_user_embeddings_{Config.MODEL_TYPE}.pkl'
            enhanced_embeddings_path = os.path.join(Config.MODEL_SAVE_PATH, enhanced_embeddings_filename)
            enhanced_embeddings = compute_enhanced_user_embeddings(
                behavior_model=model,
                attribute_model=attribute_model,
                fusion_model=fusion_model,
                user_sequences=user_sequences,
                user_attributes=user_attributes,
                url_mappings=url_mappings,
                attribute_info=attribute_info,
                location_model=location_model,
                user_location_sequences=user_location_sequences,
                base_station_mappings=base_station_mappings,
                location_weights=location_weights,
                location_processor=location_processor,
                save_path=enhanced_embeddings_path
            )
        else:
            # è®¡ç®—åŸºç¡€ç”¨æˆ·åµŒå…¥
            user_embeddings_filename = 'user_embeddings.pkl' if Config.MODEL_TYPE == 'item2vec' else 'user_embeddings_node2vec.pkl'
            user_embeddings_path = os.path.join(Config.MODEL_SAVE_PATH, user_embeddings_filename)
            compute_user_embeddings(model, user_sequences, url_mappings, user_embeddings_path)
    elif args.mode == 'compute_embeddings': # å¦‚æœæ˜¯compute_embeddingsæ¨¡å¼ä½†æ¡ä»¶æœªæ»¡è¶³ï¼Œç»™å‡ºæç¤º
        print("æ¨¡å‹ã€ç”¨æˆ·åºåˆ—æˆ–URLæ˜ å°„æœªå‡†å¤‡å¥½ï¼Œæ— æ³•è®¡ç®—ç”¨æˆ·åµŒå…¥ã€‚è¯·ç¡®ä¿ï¼š")
        print("1. å·²è¿è¡Œæ•°æ®é¢„å¤„ç†ã€‚")
        print("2. å·²æœ‰è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆæˆ–é€šè¿‡ --model_path æŒ‡å®šï¼‰ã€‚")
    
    # æ–°ç”¨æˆ·å‘é‡è®¡ç®—æ¨¡å¼
    elif args.mode == 'compute_new_users':
        print("="*50)
        print("æ–°ç”¨æˆ·å‘é‡è®¡ç®—æ¨¡å¼")
        print("="*50)
        
        # å¦‚æœæ¨¡å‹æœªåŠ è½½ï¼Œå°è¯•åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
        if model is None or url_mappings is None:
            print("å°è¯•åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹...")
            
            # åŠ è½½URLæ˜ å°„
            url_mappings_path = os.path.join(Config.PROCESSED_DATA_PATH, "url_mappings.pkl")
            if os.path.exists(url_mappings_path):
                with open(url_mappings_path, 'rb') as f:
                    url_mappings = pickle.load(f)
                print(f"URLæ˜ å°„åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(url_mappings['url_to_id'])} ä¸ªURL")
            else:
                print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°URLæ˜ å°„æ–‡ä»¶ {url_mappings_path}")
                return
            
            # åŠ è½½è¡Œä¸ºæ¨¡å‹
            behavior_model_path = os.path.join(Config.MODEL_SAVE_PATH, f"best_{Config.MODEL_TYPE}_model.pth")
            if not os.path.exists(behavior_model_path):
                behavior_model_path = os.path.join(Config.MODEL_SAVE_PATH, f"{Config.MODEL_TYPE}_model.pth")
            
            if os.path.exists(behavior_model_path):
                vocab_size = len(url_mappings['url_to_id'])
                
                if Config.MODEL_TYPE == 'item2vec':
                    model = Item2Vec(vocab_size, Config.EMBEDDING_DIM)
                else:  # node2vec
                    model = Node2Vec(vocab_size, Config.EMBEDDING_DIM)
                
                # åŠ è½½æ¨¡å‹æƒé‡
                checkpoint = torch.load(behavior_model_path, map_location='cpu')
                if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                    model.load_state_dict(checkpoint['model_state_dict'])
                else:
                    model.load_state_dict(checkpoint)
                
                model.eval()
                print(f"è¡Œä¸ºæ¨¡å‹ ({Config.MODEL_TYPE}) åŠ è½½æˆåŠŸ")
            else:
                print(f"é”™è¯¯ï¼šæœªæ‰¾åˆ°è¡Œä¸ºæ¨¡å‹æ–‡ä»¶ {behavior_model_path}")
                return
            
            # åŠ è½½å±æ€§ç›¸å…³æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if Config.ENABLE_ATTRIBUTES:
                attribute_info_path = os.path.join(Config.PROCESSED_DATA_PATH, "attribute_info.pkl")
                if os.path.exists(attribute_info_path):
                    with open(attribute_info_path, 'rb') as f:
                        attribute_info = pickle.load(f)
                    print("å±æ€§ä¿¡æ¯åŠ è½½æˆåŠŸ")
                    
                    # åŠ è½½å±æ€§æ¨¡å‹
                    attribute_model_path = os.path.join(Config.MODEL_SAVE_PATH, "best_attribute_models.pth")
                    if not os.path.exists(attribute_model_path):
                        attribute_model_path = os.path.join(Config.MODEL_SAVE_PATH, "attribute_models.pth")
                    
                    if os.path.exists(attribute_model_path):
                        attribute_model, fusion_model = load_attribute_models(attribute_model_path, attribute_info)
                        print("å±æ€§æ¨¡å‹å’Œèåˆæ¨¡å‹åŠ è½½æˆåŠŸ")
                    else:
                        print("è­¦å‘Šï¼šæœªæ‰¾åˆ°å±æ€§æ¨¡å‹æ–‡ä»¶ï¼Œå°†è·³è¿‡å±æ€§å‘é‡è®¡ç®—")
                else:
                    print("è­¦å‘Šï¼šæœªæ‰¾åˆ°å±æ€§ä¿¡æ¯æ–‡ä»¶ï¼Œå°†è·³è¿‡å±æ€§å‘é‡è®¡ç®—")
            
            # åŠ è½½ä½ç½®ç›¸å…³æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if Config.ENABLE_LOCATION:
                # åŠ è½½åŸºç«™æ˜ å°„
                base_station_mappings_path = os.path.join(Config.PROCESSED_DATA_PATH, "base_station_mappings.pkl")
                if os.path.exists(base_station_mappings_path):
                    with open(base_station_mappings_path, 'rb') as f:
                        base_station_mappings = pickle.load(f)
                    print(f"åŸºç«™æ˜ å°„åŠ è½½æˆåŠŸï¼ŒåŒ…å« {len(base_station_mappings['base_station_to_id'])} ä¸ªåŸºç«™")
                    
                    # åŠ è½½ä½ç½®æ¨¡å‹
                    location_model_path = os.path.join(Config.MODEL_SAVE_PATH, f"location_{Config.LOCATION_MODEL_TYPE}_model.pth")
                    if os.path.exists(location_model_path):
                        vocab_size = len(base_station_mappings['base_station_to_id'])
                        
                        if Config.LOCATION_MODEL_TYPE == 'item2vec':
                            location_model = Item2Vec(vocab_size, Config.LOCATION_EMBEDDING_DIM)
                        else:  # node2vec
                            location_model = Node2Vec(vocab_size, Config.LOCATION_EMBEDDING_DIM)
                        
                        # åŠ è½½ä½ç½®æ¨¡å‹æƒé‡
                        checkpoint = torch.load(location_model_path, map_location='cpu')
                        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                            location_model.load_state_dict(checkpoint['model_state_dict'])
                        else:
                            location_model.load_state_dict(checkpoint)
                        
                        location_model.eval()
                        print(f"ä½ç½®æ¨¡å‹ ({Config.LOCATION_MODEL_TYPE}) åŠ è½½æˆåŠŸ")
                        
                        # åˆ›å»ºä½ç½®å¤„ç†å™¨
                        location_processor = LocationProcessor(Config)
                        if Config.BASE_STATION_FEATURE_MODE != "none":
                            location_processor.load_base_station_features(Config.LOCATION_FEATURES_PATH)
                    else:
                        print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°ä½ç½®æ¨¡å‹æ–‡ä»¶ {location_model_path}")
                else:
                    print("è­¦å‘Šï¼šæœªæ‰¾åˆ°åŸºç«™æ˜ å°„æ–‡ä»¶ï¼Œå°†è·³è¿‡ä½ç½®å‘é‡è®¡ç®—")
        
        # è®¡ç®—æ–°ç”¨æˆ·å‘é‡
        new_user_embeddings_filename = f'new_user_embeddings_{Config.MODEL_TYPE}.pkl'
        new_user_embeddings_path = os.path.join(Config.MODEL_SAVE_PATH, new_user_embeddings_filename)
        
        new_user_embeddings = compute_new_user_embeddings(
            behavior_model=model,
            attribute_model=attribute_model,
            fusion_model=fusion_model,
            url_mappings=url_mappings,
            attribute_info=attribute_info,
            base_station_mappings=base_station_mappings,
            location_model=location_model,
            location_processor=location_processor,
            new_user_behavior_path=args.new_user_behavior_path,
            new_user_attribute_path=args.new_user_attribute_path,
            new_user_location_path=args.new_user_location_path,
            save_path=new_user_embeddings_path
        )
        
        if new_user_embeddings:
            print(f"æ–°ç”¨æˆ·å‘é‡è®¡ç®—å®Œæˆï¼å…±è®¡ç®—äº† {len(new_user_embeddings)} ä¸ªæ–°ç”¨æˆ·çš„å‘é‡")
        else:
            print("æœªèƒ½è®¡ç®—ä»»ä½•æ–°ç”¨æˆ·å‘é‡ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®")
    
    print("="*50)
    print("ç¨‹åºæ‰§è¡Œå®Œæˆï¼")
    print("="*50)

if __name__ == "__main__":
    main() 